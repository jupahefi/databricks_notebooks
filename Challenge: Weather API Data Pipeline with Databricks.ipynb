{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf7e00f2-a6a7-47cb-bda1-e1951bb72491",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook represents a PoC for a Data Pipeline with Ingestion, Storage, Processing, Analysis, Exploration and Visualization.\n",
    "\n",
    "We use Databricks (with PySpark), Azure Storage Account and Unity Catalog with External locations for this purpose.\n",
    "\n",
    "Practices: DRY, KISS, Clean Code Principles and Error Handling.\n",
    "Paradigm: Basic functional programming\n",
    "\n",
    "Pipeline specifications\n",
    "1. Process last 7-days data into the database table(s) when running the pipeline for the first time. Consequent runs must insert only the new records. **This is not possible due to outdated data - In a first exploration I realized that the data is too old; thus, it is not possible to process only last 7 days. Also considering the low volumne of the data, full ingest will be made.**\n",
    "2. Re-running the pipeline multiple times, must not result in duplicate records in the table. **Ok**\n",
    "3. Feel free to assume other details as required. Document your assumptions. **All assumptions will be commented on the code and if necessary, in a text block of the notebook**\n",
    "\n",
    "In terms of doing incremental data, minor changes would be needed to use a temporary table that can be compared with the final one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e3eabbf-5383-4577-a45b-c7c7fa196a33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ingestion: Fetch Weather Data for a Single Station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe0554d1-3d79-4ff6-b416-13e6c6fb5d51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fe1df6-0e3d-4ec2-8d05-e39a2fed2f01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def fetch_raw_weather_data(station_id: str, observations_url_template: str) -> dict:\n",
    "    \"\"\"Fetch raw weather data for a specific station and return it as raw JSON.\"\"\"\n",
    "    observations_url = observations_url_template.format(station_id=station_id)\n",
    "    response = requests.get(observations_url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def save_raw_data_to_storage(spark: SparkSession, raw_data: dict, raw_storage_path: str) -> None:\n",
    "    \"\"\"Ingest raw JSON data into the storage system in Delta format without any transformation.\"\"\"\n",
    "    # Convert the raw JSON data into a JSON string format suitable for Delta Lake\n",
    "    raw_json_str = json.dumps(raw_data)\n",
    "    \n",
    "    # Create a DataFrame from the JSON string\n",
    "    raw_df = spark.createDataFrame([(raw_json_str,)], [\"value\"])\n",
    "    \n",
    "    # Save the raw data to the specified storage path (Delta or raw container)\n",
    "    raw_df.write.format(\"delta\").mode(\"overwrite\").save(raw_storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89da8109-f463-405e-85b5-755fe681c2f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f903221e-7363-47ac-a5c3-efa4cf5302b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set station ID and API URL template\n",
    "station_id = \"0112W\"\n",
    "observations_url_template = \"https://api.weather.gov/stations/{station_id}/observations\"\n",
    "\n",
    "# Fetch the raw weather data (without any extraction or transformation)\n",
    "raw_weather_data = fetch_raw_weather_data(station_id, observations_url_template)\n",
    "\n",
    "# Save the raw JSON data to storage in Delta format\n",
    "raw_storage_path = \"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/weather_observations_raw\"\n",
    "save_raw_data_to_storage(spark, raw_weather_data, raw_storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c1e157-8568-42c8-9f71-19aa748b3144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Storage: Load Data into Delta Table with External Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d5b1c4d-9caf-4020-8cca-032b411aec39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0216bdc5-c373-48f3-9545-ca1de047ed5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_external_table_if_not_exists(spark: SparkSession, table_name: str, delta_storage_path: str) -> None:\n",
    "    \"\"\"Create an external Delta table in Unity Catalog if it does not already exist.\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS {table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{delta_storage_path}'\n",
    "    \"\"\")\n",
    "\n",
    "def save_raw_data_to_unity_table(spark: SparkSession, raw_data: dict, table_name: str, delta_storage_path: str) -> None:\n",
    "    \"\"\"Save the ingested raw JSON data to Unity Catalog in the specified Delta table.\"\"\"\n",
    "\n",
    "    create_external_table_if_not_exists(spark, table_name, delta_storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b8d7ad8-e9e4-44a3-9d0f-5b437c57ae7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c55a53-91fc-4761-abe2-80d37507e80e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the table name and Delta storage path\n",
    "table_name = \"unity_catalog.lab.weather_observations_raw\"\n",
    "delta_storage_path = \"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/weather_observations_raw\"\n",
    "\n",
    "# Fetch the raw weather data (this can be from the previous ingestion stage)\n",
    "raw_weather_data = fetch_raw_weather_data(station_id, observations_url_template)\n",
    "\n",
    "# Save the raw data to Unity Catalog as a Delta table\n",
    "save_raw_data_to_unity_table(spark, raw_weather_data, table_name, delta_storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf98624-50c9-4b87-b05d-6aa6f58558c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Processing: Preparing, cleaning and structuring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c575622e-a596-47a0-81d2-5864eef7414b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS unity_catalog.lab.weather_observations_cleaned;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48a550b-6009-4d84-9772-7d03e6b996da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE unity_catalog.lab.weather_observations_cleaned\n",
    "LOCATION 'abfss://staging@cs2100320032141b0ad.dfs.core.windows.net/weather_observations_cleaned'\n",
    "AS\n",
    "WITH \n",
    "parsed AS (\n",
    "  SELECT\n",
    "    feature.properties.station AS station_id,\n",
    "    feature.id AS station_name,\n",
    "    feature.geometry.coordinates[0] AS longitude,\n",
    "    feature.geometry.coordinates[1] AS latitude,\n",
    "    feature.properties.timestamp AS observation_timestamp,\n",
    "    'UTC' AS station_timezone,  -- Assuming UTC\n",
    "    round(cast(feature.properties.temperature.value AS double), 2) AS temperature,\n",
    "    round(cast(feature.properties.windSpeed.value AS double), 2) AS wind_speed,\n",
    "    round(cast(feature.properties.relativeHumidity.value AS double), 2) AS humidity\n",
    "  FROM unity_catalog.lab.weather_observations_raw\n",
    "  LATERAL VIEW EXPLODE(from_json(get_json_object(value, '$.features'), \n",
    "    'ARRAY<STRUCT<\n",
    "      id: STRING, \n",
    "      geometry: STRUCT<coordinates: ARRAY<DOUBLE>>, \n",
    "      properties: STRUCT<\n",
    "        station: STRING, \n",
    "        timestamp: STRING, \n",
    "        temperature: STRUCT<value: DOUBLE>, \n",
    "        windSpeed: STRUCT<value: DOUBLE>, \n",
    "        relativeHumidity: STRUCT<value: DOUBLE>\n",
    "      >\n",
    "    >>')) AS feature\n",
    ")\n",
    "SELECT * FROM parsed\n",
    "WHERE station_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b90710-2b4d-4d3e-96cb-ff99f9a694ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exploration: Know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a14570-45f3-48f6-ab68-1929f7e7d94f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM unity_catalog.lab.weather_observations_cleaned;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02a94d3-03ca-4c70-a763-6926ca789095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check duplications\n",
    "SELECT DISTINCT * FROM unity_catalog.lab.weather_observations_cleaned;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff1c619-d126-42ec-ac8b-70bcc4e0a071",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Analysis: Required metrics in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f76e34-a246-4905-a625-50d597c47f3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Average observed temperature for the last week (Monday-Sunday)\n",
    "Note: as the data is outdated, the date filter was commented to explore the analyzed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2399b610-d58b-446a-ac15-1ab0fd478896",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE unity_catalog.lab.weather_observations_avg\n",
    "LOCATION 'abfss://curated@cs2100320032141b0ad.dfs.core.windows.net/weather_observations_avg'\n",
    "AS\n",
    "SELECT AVG(temperature) AS avg_temperature_last_week\n",
    "FROM unity_catalog.lab.weather_observations_cleaned\n",
    "/*WHERE observation_timestamp >= DATEADD(day, -7, CURRENT_DATE) -- Last 7 days from today\n",
    "  AND observation_timestamp <= CURRENT_DATE;*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87d6c6d-0a33-4555-b8a4-e87a898f7f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE unity_catalog.lab.weather_observations_max_speed\n",
    "LOCATION 'abfss://curated@cs2100320032141b0ad.dfs.core.windows.net/weather_observations_max_speed'\n",
    "AS\n",
    "WITH \n",
    "ranked_observations AS (\n",
    "  SELECT \n",
    "    station_id, \n",
    "    observation_timestamp, \n",
    "    wind_speed, \n",
    "    ROW_NUMBER() OVER (PARTITION BY station_id ORDER BY observation_timestamp) AS row_num\n",
    "  FROM unity_catalog.lab.weather_observations_cleaned\n",
    "  /*WHERE observation_timestamp >= DATEADD(day, -7, CURRENT_DATE)\n",
    "    AND observation_timestamp <= CURRENT_DATE*/\n",
    "),\n",
    "wind_speed_changes AS (\n",
    "  SELECT \n",
    "    a.station_id,\n",
    "    a.observation_timestamp,\n",
    "    ABS(a.wind_speed - b.wind_speed) AS wind_speed_change\n",
    "  FROM ranked_observations a\n",
    "  INNER JOIN ranked_observations b\n",
    "    ON a.station_id = b.station_id\n",
    "      AND a.row_num = b.row_num + 1 -- Consecutive observations\n",
    ")\n",
    "\n",
    "SELECT MAX(wind_speed_change) AS max_wind_speed_change\n",
    "FROM wind_speed_changes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5248e3b6-1c25-4b99-8abb-765b6c61d57e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Visualization: Databricks visualizations to show Curated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e090eee0-1bda-44d7-9f09-e94c79e34206",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Note: Select visualization tab for each code block in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c3bea0-2e88-40d3-a43c-6e578fee4e4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql\nselect * from unity_catalog.lab.weather_observations_avg;",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "counterColName": "avg_temperature_last_week",
           "counterLabel": "°C",
           "rowNumber": 1,
           "stringDecChar": ".",
           "stringDecimal": 1,
           "stringThouSep": ",",
           "targetRowNumber": 1,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "dfe137af-71ae-4aef-bb7b-14ed123a2f07",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 15.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from unity_catalog.lab.weather_observations_avg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e80c028-8ae2-4b8e-a142-173d1fd465e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql\nselect * from unity_catalog.lab.weather_observations_max_speed",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "counterColName": "max_wind_speed_change",
           "counterLabel": "km/h",
           "rowNumber": 1,
           "stringDecChar": ".",
           "stringDecimal": 1,
           "stringThouSep": ",",
           "targetRowNumber": 1,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "2fab84b9-d9e6-4cce-a045-b8ba0bdb59be",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 16.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from unity_catalog.lab.weather_observations_max_speed"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2095135864188646,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Challenge: Weather API Data Pipeline with Databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
