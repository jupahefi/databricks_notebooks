{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be863c14-5c77-4012-ac98-02874ebffd1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Ruta de la imagen en ADLS Gen2\n",
    "image_path = \"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/images/z.png\"\n",
    "\n",
    "# Leer la imagen desde ADLS Gen2 como un archivo binario\n",
    "image_df = spark.read.format(\"binaryFile\").load(image_path)\n",
    "\n",
    "# Definir una función UDF para convertir el contenido binario en un array multidimensional\n",
    "def image_to_array(image_binary):\n",
    "    image = Image.open(io.BytesIO(image_binary))\n",
    "    return np.array(image).tolist()\n",
    "\n",
    "# Registrar la UDF\n",
    "image_to_array_udf = udf(image_to_array, ArrayType(ArrayType(ArrayType(IntegerType()))))\n",
    "\n",
    "# Aplicar la UDF para convertir la imagen a un array\n",
    "image_array_df = image_df.withColumn(\"image_array\", image_to_array_udf(col(\"content\")))\n",
    "\n",
    "# Seleccionar solo la columna del array\n",
    "image_array_df = image_array_df.select(\"image_array\")\n",
    "\n",
    "# Mostrar el array (truncado para que no sea muy largo)\n",
    "image_array_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba290d3a-47c1-4b81-8946-5c8e1356a94d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, explode, array\n",
    "from pyspark.sql.types import DoubleType, IntegerType, ArrayType\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Explosión de la imagen en píxeles individuales (aplanar la estructura)\n",
    "pixels_df: DataFrame = image_array_df.select(explode(\"image_array\").alias(\"row\")) \\\n",
    "                          .select(explode(\"row\").alias(\"pixel\"))\n",
    "\n",
    "# Descomponer cada píxel en sus componentes RGB y convertir a double\n",
    "pixels_df = pixels_df.select(\n",
    "    col(\"pixel\")[0].cast(DoubleType()).alias(\"R\"),\n",
    "    col(\"pixel\")[1].cast(DoubleType()).alias(\"G\"),\n",
    "    col(\"pixel\")[2].cast(DoubleType()).alias(\"B\")\n",
    ")\n",
    "\n",
    "# Usar VectorAssembler para crear la columna 'features' que Spark KMeans espera\n",
    "assembler = VectorAssembler(inputCols=[\"R\", \"G\", \"B\"], outputCol=\"features\")\n",
    "pixels_df = assembler.transform(pixels_df).select(\"features\")\n",
    "\n",
    "# Aplicar KMeans para agrupar los píxeles\n",
    "kmeans: KMeans = KMeans(k=5, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model: KMeansModel = kmeans.fit(pixels_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66aaa315-5faa-471c-8399-765a12f62ba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los clusters (predicciones)\n",
    "clustered_pixels_df: DataFrame = model.transform(pixels_df)\n",
    "\n",
    "# Obtener las dimensiones originales de la imagen\n",
    "image_dimensions = image_array_df.select(\"image_array\").first()[0]\n",
    "image_height = len(image_dimensions)\n",
    "image_width = len(image_dimensions[0])\n",
    "\n",
    "# Reasignar los colores basados en los centroides\n",
    "centroids = model.clusterCenters()\n",
    "\n",
    "# UDF para asignar el color basado en el centroide del cluster\n",
    "def assign_color(cluster: int) -> list:\n",
    "    return [int(c) for c in centroids[cluster]]\n",
    "\n",
    "assign_color_udf = udf(assign_color, ArrayType(IntegerType()))\n",
    "\n",
    "# Aplicar la UDF para obtener los nuevos colores\n",
    "colored_pixels_df: DataFrame = clustered_pixels_df.withColumn(\"new_pixel\", assign_color_udf(col(\"cluster\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f48bc7-4f53-4fed-bae3-fa41d016ff06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window  # <--- Esto es crucial\n",
    "from pyspark.sql.types import BinaryType, StructType, StructField, IntegerType, ArrayType\n",
    "\n",
    "# Agregar un índice para rastrear la posición de los píxeles\n",
    "indexed_pixels_df = colored_pixels_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "# Calcular las dimensiones de la imagen\n",
    "image_dimensions = image_array_df.select(\"image_array\").first()[0]\n",
    "image_height = len(image_dimensions)\n",
    "image_width = len(image_dimensions[0])\n",
    "\n",
    "# Reconstruir la imagen usando la indexación\n",
    "window_spec = Window.orderBy(\"index\")\n",
    "reconstructed_pixels_df = indexed_pixels_df.withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "    .withColumn(\"row_group\", F.floor((F.col(\"row_num\") - 1) / image_width)) \\\n",
    "    .groupBy(\"row_group\") \\\n",
    "    .agg(F.collect_list(\"new_pixel\").alias(\"row_pixels\"))\n",
    "\n",
    "# Crear una estructura binaria de la imagen usando solo Spark\n",
    "def encode_row_as_bytes(row_pixels):\n",
    "    # Convertir cada fila de píxeles en bytes y luego combinar\n",
    "    return bytes([component for pixel in row_pixels for component in pixel])\n",
    "\n",
    "encode_row_as_bytes_udf = F.udf(encode_row_as_bytes, BinaryType())\n",
    "\n",
    "binary_image_df = reconstructed_pixels_df.withColumn(\"binary_row\", encode_row_as_bytes_udf(F.col(\"row_pixels\")))\n",
    "\n",
    "# Combinar las filas binarias para formar la imagen completa\n",
    "combined_binary_image = binary_image_df.agg(F.collect_list(\"binary_row\").alias(\"binary_image\")).collect()[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d25520-d8dd-4b0c-b3f1-228eca2f0457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imprimir las dimensiones calculadas\n",
    "print(f\"Dimensiones de la imagen calculadas: Ancho = {image_width}, Altura = {image_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da4de41-d5e0-4764-88ba-4cb64bdbcbe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear un DataFrame con el contenido binario\n",
    "binary_image_spark_df = spark.createDataFrame([(combined_binary_image,)], [\"image_array\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae97050d-426a-4d50-8f07-09d6a0004e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Obtener la primera fila del DataFrame\n",
    "first_row = binary_image_spark_df.first()\n",
    "\n",
    "# Obtener el campo 'image_array' desde la primera fila del DataFrame\n",
    "image_array = first_row['image_array']\n",
    "\n",
    "# Convertir todos los elementos de image_array a una lista de bytes\n",
    "list_of_bytes = [bytes(ba) for ba in image_array]\n",
    "\n",
    "# Combinar todos los bytes en un único bytearray\n",
    "combined_bytes = b''.join(list_of_bytes)\n",
    "\n",
    "# Asegurarte de que el tamaño de combined_bytes coincida con el esperado\n",
    "expected_size = image_width * image_height * 3  # Ancho * Alto * 3 (para RGB)\n",
    "if len(combined_bytes) != expected_size:\n",
    "    print(f\"Advertencia: el tamaño de los datos no coincide. Esperado: {expected_size}, Real: {len(combined_bytes)}\")\n",
    "\n",
    "# Ahora intenta convertirlo en una imagen\n",
    "try:\n",
    "    image = Image.frombytes('RGB', (1920, 1080), combined_bytes)\n",
    "    image.show()  # Mostrar la imagen para ver si se carga correctamente\n",
    "    image.save('imagen_resultante.png')  # Guardar imagen en el almacenamiento\n",
    "    print(\"Imagen guardada exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al intentar crear la imagen: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad730544-baf6-45d9-85cc-c179bd00d8ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Show saved image\n",
    "image_path = 'imagen_resultante.png'\n",
    "display(Image(filename=image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "077bf061-dc96-4787-9550-42d134a887e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imagen Original\n",
    "![z.png](./z.png \"z.png\")\n",
    "\n",
    "### Imagen Con Clustering (k=5)\n",
    "![download.png](./download.png \"download.png\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Image Clustering",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
