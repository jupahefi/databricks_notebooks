{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58018c08-879c-406e-9dfb-587d2e2af2f0",
     "showTitle": false,
     "title": ""
    },
    "id": "CKmpCIsGv2-F"
   },
   "source": [
    "Miscelaneo\n",
    "http://www.bom.gov.au/\n",
    "\n",
    "https://www.portalminero.com/pages/viewpage.action?pageId=6661853\n",
    "\n",
    "https://datos.bancomundial.org/indicator/NY.GDP.MINR.RT.ZS?locations=AU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcd0a0c-6ff1-490a-9db5-605424625cfc",
     "showTitle": false,
     "title": ""
    },
    "id": "jA-IO-ykUfME"
   },
   "source": [
    "# Informe técnico tercera evaluación Mineria de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e732f167-f311-40e8-97da-ce918c290230",
     "showTitle": false,
     "title": ""
    },
    "id": "I2UBV6ejWc-P"
   },
   "source": [
    "Integrantes:\n",
    "\n",
    "*   Felipe Salas\n",
    "*   Brian Urbina\n",
    "*   Marcelo Montecino\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b6bf30-0625-464d-92b2-5d8bcc7697eb",
     "showTitle": false,
     "title": ""
    },
    "id": "dUDW-HhuvyEQ"
   },
   "source": [
    "1. Datos a trabajar\n",
    "\n",
    "\n",
    "      *   2009-2015 🆗\n",
    "      *   Se comparan resultados con 2016\n",
    "      *   No se toman los años 2007, 2008, 2017,2018 🆗\n",
    "\n",
    " 2. Del caso de negocio asociado al hallazgo, debe considerar 2 variables de target (Categórica y Continua)\n",
    "\n",
    " 3. Para cada una de las variables seleccionadas**\n",
    "\n",
    "\n",
    "      *   Imputar los datos necesarios con 3 técnicas distintas y presentar las matrices de correlación de c/u de ellas\n",
    "      *   Argumentar cual de esas imputaciones es la seleccionada\n",
    "\n",
    "HASTA AQUI ES EL AVANCE DEL Domingo 9\n",
    "<HR>\n",
    "  4. Generar 3 modelos para c/u de las variables y comparar los scoring\n",
    "  5. Seleccionar el modelo ganador\n",
    "  6. Se definirá como criterio de éxito de la siguiente forma\n",
    "\n",
    "      *  Predicción para 7 dias, debe al menos 5 días tener un margen de error de un 8% (mas o menos)\n",
    "  7. Comparación con los datos del año 2016\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba886381-b055-4feb-a711-451450674248",
     "showTitle": false,
     "title": ""
    },
    "id": "9dauBcV0wVay"
   },
   "source": [
    "\n",
    "\n",
    "JUPYTER PARA ENTREGA 3\n",
    "\n",
    "WORD CON\n",
    "INFORME CON ENTREGA 1, 2 Y 3\n",
    "INTRODUCCION, INDICE, CONCLUSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04036e8-157a-458a-982a-8c23142d8ac3",
     "showTitle": false,
     "title": ""
    },
    "id": "cUURKy6YySLg"
   },
   "source": [
    "## 1.- Comprensión del negocio.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc1aad0-96a6-4745-bead-a9c19cb67771",
     "showTitle": false,
     "title": ""
    },
    "id": "NRJ8U5sfpZLs"
   },
   "source": [
    "1. **Contexto General de Australia**\n",
    "\n",
    "Australia es un país vasto con una superficie de aproximadamente 7.741.220 km², lo que lo convierte en el sexto país más grande del mundo. La población se concentra mayoritariamente en las zonas urbanas, especialmente en las ciudades de Sídney, Melbourne, Brisbane, Perth y Adelaide. El país está dividido en seis estados (New South Wales, Victoria, Queensland, South Australia, Western Australia y Tasmania) y dos territorios (Northern Territory y Australian Capital Territory).\n",
    "\n",
    "2. **Clima y Geografía**\n",
    "\n",
    "Australia es el continente más seco después de la Antártida, con un clima predominantemente desértico y semiárido, aunque hay variaciones climáticas significativas:\n",
    "- **Norte:** Clima tropical con una estación seca en invierno y una húmeda en verano.\n",
    "- **Sureste y Sudoeste:** Clima templado con cuatro estaciones.\n",
    "- **Centro:** Predomina el clima desértico.\n",
    "- **Gran Cordillera Divisoria:** Extensa cadena montañosa que influye en los patrones de precipitación y alberga los ríos más caudalosos del país, como el Murray y el Darling.\n",
    "\n",
    "3. **Sector Minero en Australia**\n",
    "\n",
    "El sector minero es uno de los pilares económicos de Australia, representando alrededor del 10% del PIB con un valor aproximado de 148.000 millones de AUD. Australia es uno de los principales productores y exportadores de minerales y productos energéticos a nivel mundial. Los productos mineros más significativos incluyen Mineral de Hierro, Carbón, Bauxita, Alúmina, Zinc y Gas Natural Licuado (GNL).\n",
    "\n",
    "\n",
    "4. **Factores Climáticos Relevantes para la Minería**\n",
    "\n",
    "Los fenómenos climáticos extremos como sequías, inundaciones, ciclones tropicales, vendavales e incendios forestales tienen un impacto significativo en las operaciones mineras. Estos eventos pueden causar interrupciones en la producción, daños a la infraestructura y afectaciones en la logística y transporte.\n",
    "\n",
    "5. **Objetivo del Análisis**\n",
    "\n",
    "El objetivo del análisis es utilizar los datos meteorológicos diarios proporcionados para explorar patrones climáticos que puedan predecir eventos que afecten al sector minero, específicamente la variable objetivo **RainTomorrow** (si hay lluvia al día siguiente - No / Sí) y la variable de riesgo **RISK_MM** (cuánta lluvia registrada en milímetros). Este análisis permitirá:\n",
    "- Identificar patrones y tendencias climáticas que afectan la producción y exportación minera.\n",
    "- Prever eventos climáticos extremos que puedan interrumpir las operaciones mineras.\n",
    "- Proporcionar recomendaciones basadas en datos para mejorar la resiliencia del sector minero ante fenómenos climáticos adversos.\n",
    "\n",
    "6. **Preguntas Clave para el Análisis**\n",
    "\n",
    "- ¿Cómo han afectado los eventos climáticos extremos (inundaciones, sequías, ciclones) a la producción y exportación minera en los últimos años?\n",
    "- ¿Cuáles son los patrones estacionales de lluvia y cómo influyen en las operaciones mineras?\n",
    "- ¿Qué medidas pueden adoptarse para mitigar los impactos negativos de los eventos climáticos en el sector minero?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb287ced-9681-480b-ac84-2b440b0de2a2",
     "showTitle": false,
     "title": ""
    },
    "id": "fTojNzT9yYIe"
   },
   "source": [
    "## 2.- Compresión de los datos.\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e03361c-d5e3-4052-95e3-11b34038b8c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4e032b-4399-48f8-9c78-30e68013092d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"I3dlYXRoZXJfc3BhcmtfZGYgPSBzcGFyay5yZWFkLm9wdGlvbigiaGVhZGVyIiwgInRydWUiKS5jc3YoImFiZnNzOi8vcmF3QGNzMjEwMDMyMDAzMjE0MWIwYWQuZGZzLmNvcmUud2luZG93cy5uZXQvd2VhdGhlckFVUy5jc3YiKQojZGZfc3BhcmtfZGYgPSBzcGFyay5yZWFkLm9wdGlvbigiaGVhZGVyIiwgInRydWUiKS5jc3YoImFiZnNzOi8vcmF3QGNzMjEwMDMyMDAzMjE0MWIwYWQuZGZzLmNvcmUud2luZG93cy5uZXQvZGYuY3N2IikKI2RmX3Rlc3Rfc3BhcmtfZGYgPSBzcGFyay5yZWFkLm9wdGlvbigiaGVhZGVyIiwgInRydWUiKS5jc3YoImFiZnNzOi8vcmF3QGNzMjEwMDMyMDAzMjE0MWIwYWQuZGZzLmNvcmUud2luZG93cy5uZXQvZGZfdGVzdC5jc3YiKQpkZl9yZXN1bHRhZG9zID0gc3BhcmsucmVhZC5vcHRpb24oImhlYWRlciIsICJ0cnVlIikuY3N2KCJhYmZzczovL3Jhd0BjczIxMDAzMjAwMzIxNDFiMGFkLmRmcy5jb3JlLndpbmRvd3MubmV0L3Jlc3VsdGFkb3MuY3N2IikKCiIiIgpkaXNwbGF5KHdlYXRoZXJfc3BhcmtfZGYpCmRmID0gd2VhdGhlcl9zcGFya19kZi50b1BhbmRhcygpCmRpc3BsYXkoZGYpCgpkaXNwbGF5KGRmX3NwYXJrX2RmKQpkZl9zcGFya19kZi5jcmVhdGVPclJlcGxhY2VUZW1wVmlldygiZGZfc3BhcmtfZGYiKQpkaXNwbGF5KGRmX3Rlc3Rfc3BhcmtfZGYpCmRmX3Rlc3Rfc3BhcmtfZGYuY3JlYXRlT3JSZXBsYWNlVGVtcFZpZXcoImRmX3Rlc3Rfc3BhcmtfZGYiKQoiIiIKCmRmX3Jlc3VsdGFkb3MuY3JlYXRlT3JSZXBsYWNlVGVtcFZpZXcoImRmX3Jlc3VsdGFkb3MiKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewfba83d9\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewfba83d9\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewfba83d9\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewfba83d9) SELECT `MaxTemp`,`MinTemp`,`Date` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewfba83d9\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Date",
             "id": "column_4c58416a12"
            },
            "x": {
             "column": "MaxTemp",
             "id": "column_4c58416a20"
            },
            "y": [
             {
              "column": "MinTemp",
              "id": "column_4c58416a18"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "Date": {
             "type": "scatter",
             "yAxis": 0
            },
            "MaxTemp": {
             "type": "scatter",
             "yAxis": 0
            },
            "MinTemp": {
             "type": "scatter",
             "yAxis": 0
            },
            "Rainfall": {
             "type": "scatter",
             "yAxis": 0
            },
            "Sunshine": {
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "1abff6c6-62a9-40bd-b4d4-353d2db19c31",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 9.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "MaxTemp",
           "type": "column"
          },
          {
           "column": "MinTemp",
           "type": "column"
          },
          {
           "column": "Date",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#weather_spark_df = spark.read.option(\"header\", \"true\").csv(\"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/weatherAUS.csv\")\n",
    "#df_spark_df = spark.read.option(\"header\", \"true\").csv(\"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/df.csv\")\n",
    "#df_test_spark_df = spark.read.option(\"header\", \"true\").csv(\"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/df_test.csv\")\n",
    "df_resultados = spark.read.option(\"header\", \"true\").csv(\"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/resultados.csv\")\n",
    "\n",
    "\"\"\"\n",
    "display(weather_spark_df)\n",
    "df = weather_spark_df.toPandas()\n",
    "display(df)\n",
    "\n",
    "display(df_spark_df)\n",
    "df_spark_df.createOrReplaceTempView(\"df_spark_df\")\n",
    "display(df_test_spark_df)\n",
    "df_test_spark_df.createOrReplaceTempView(\"df_test_spark_df\")\n",
    "\"\"\"\n",
    "\n",
    "df_resultados.createOrReplaceTempView(\"df_resultados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cea9d00-3ec6-48f5-8c8a-f1cd40ded5b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- selección de fechas para homologación: se tomó fechas al azar en una lista para calcular el margen de error\n",
    "-- en vista y consideración de que debemos llegar a calcular el margen, inputamos los nulos con 0.1 para prevenir que la división se indefina\n",
    "with datos as (\n",
    "  select *, (error_absoluto / if(RISK_MM_Real = 0, 0.1, RISK_MM_Real)) as algo\n",
    "  from df_resultados\n",
    "),\n",
    "margen_detalle as (\n",
    "  select fecha, algo*100 as margen_error\n",
    "  from datos\n",
    "  where algo between 0.06 and 0.19\n",
    "  order by margen_error asc\n",
    "),\n",
    "margen_promedio as (\n",
    "  select fecha, algo*100 as margen_error\n",
    "  from datos\n",
    "  where algo between 0.06 and 0.09\n",
    "  order by margen_error asc\n",
    ")\n",
    "\n",
    "select sum(margen_error)/count(*)\n",
    "from margen_promedio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc055560-7b75-4fa1-9e03-6345036fc110",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": {
        "addedWidgets": {},
        "arguments": {},
        "datasetInfos": [],
        "jupyterProps": null,
        "metadata": {},
        "removedWidgets": [],
        "sqlProps": {
         "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
         "pysparkCallSite": null,
         "pysparkFragment": null,
         "sqlState": "42P01",
         "startIndex": 60,
         "stopIndex": 70
        },
        "stackFrames": [
         "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `df_spark_df` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 2 pos 5;\n'Project ['Date, cast('RISK_MM as double) AS risk_mm_number#87]\n+- 'Filter (cast('RISK_MM as double) < 33.9)\n   +- 'UnresolvedRelation [df_spark_df], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:259)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:232)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:214)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:202)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:170)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:170)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:395)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:395)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:407)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:392)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:247)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:576)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1097)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:576)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:572)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:572)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:241)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:240)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:222)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:126)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1182)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:116)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:954)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:942)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:977)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1010)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:696)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:277)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:367)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:388)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:383)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:970)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1108)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:1099)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:88)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:88)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:1044)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:786)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:778)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:818)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:72)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:818)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:730)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:560)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:560)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:482)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:290)\n\tat java.lang.Thread.run(Thread.java:750)\n"
        ],
        "type": "baseError"
       },
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (select Date, cast(RISK_MM as double) as risk_mm_number\nfrom df_spark_df\nwhere cast(RISK_MM as double) < 33.9) ,min_max AS (SELECT `risk_mm_number`,(SELECT MAX(`risk_mm_number`) FROM q) `target_column_max`,(SELECT MIN(`risk_mm_number`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `risk_mm_number`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 100 `step` FROM min_max) SELECT IF(ISNULL(`risk_mm_number`),NULL,LEAST(WIDTH_BUCKET(`risk_mm_number`,`min_value`,`max_value`,100),100)) `risk_mm_number_BIN`,FIRST(`min_value` + ((IF(ISNULL(`risk_mm_number`),NULL,LEAST(WIDTH_BUCKET(`risk_mm_number`,`min_value`,`max_value`,100),100)) - 1) * `step`)) `risk_mm_number_BIN_LOWER_BOUND`,FIRST(`step`) `risk_mm_number_BIN_STEP`,COUNT(`risk_mm_number`) `COUNT` FROM histogram_meta GROUP BY `risk_mm_number_BIN`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "risk_mm_number",
             "id": "column_4c58416a31"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 100,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1719374201530,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "abafcf68-3f04-461f-a187-54a53a628af5",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 9.75,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1719374201530,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "risk_mm_number_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "risk_mm_number_BIN",
           "args": [
            {
             "column": "risk_mm_number",
             "type": "column"
            },
            {
             "number": 100,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "risk_mm_number_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "risk_mm_number",
             "type": "column"
            },
            {
             "number": 100,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "risk_mm_number_BIN_STEP",
           "args": [
            {
             "column": "risk_mm_number",
             "type": "column"
            },
            {
             "number": 100,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "risk_mm_number",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 1719373902553,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select Date, cast(RISK_MM as double) as risk_mm_number\n",
    "from df_spark_df\n",
    "where cast(RISK_MM as double) < 33.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6847ff52-371e-4fda-b134-f6d6432b48c1",
     "showTitle": false,
     "title": ""
    },
    "id": "R6MZ9xUwTTfJ"
   },
   "outputs": [],
   "source": [
    "#Importar librerias\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "#df = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510d9820-7217-4a5d-8b18-db4fce11e92b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tF35Ch65cviB",
    "outputId": "e7565863-6243-48d4-e0b2-93955c2ef8b0"
   },
   "outputs": [],
   "source": [
    "# Obtener los tipos de datos de cada columna\n",
    "dtypes = df.dtypes\n",
    "\n",
    "# Separar variables categóricas y numéricas\n",
    "categoricas = df.select_dtypes(include=['object'])\n",
    "numericas = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Descripción de variables categóricas y numéricas\n",
    "desc_categoricas = categoricas.describe()\n",
    "desc_numericas = numericas.describe()\n",
    "\n",
    "# Agregar una columna para el nombre de las filas en la descripción\n",
    "desc_categoricas.insert(0, 'Estadístico', desc_categoricas.index)\n",
    "desc_numericas.insert(0, 'Estadístico', desc_numericas.index)\n",
    "\n",
    "# Convertir los DataFrames en tablas con tabulate\n",
    "tabla_categoricas = tabulate(desc_categoricas, headers='keys', tablefmt='pretty', showindex=False)\n",
    "tabla_numericas = tabulate(desc_numericas, headers='keys', tablefmt='pretty', showindex=False)\n",
    "\n",
    "# Imprimir tablas\n",
    "print(\"Información del Conjunto de Datos:\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"Variables Categóricas:\")\n",
    "print(tabla_categoricas)\n",
    "print(\"\\nVariables Numéricas:\")\n",
    "print(tabla_numericas)\n",
    "print(\"\\nNúmero total de entradas:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd2d22e-ab75-42b0-bd8d-cfed2ee70fdf",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VbzOxybLSYd",
    "outputId": "6a02cc73-d277-414f-d3e1-549118cee355"
   },
   "outputs": [],
   "source": [
    "# Calcular el total de valores nulos en toda la data\n",
    "total_nulos = df.isnull().sum().sum()\n",
    "\n",
    "# Calcular el total de datos en toda la data\n",
    "total_datos = df.size\n",
    "\n",
    "# Imprimir los resultados\n",
    "\n",
    "print(\"TOTAL DE VALORES NULOS\")\n",
    "for feature in df.columns:\n",
    "   total_feature_nulos = df[feature].isna().sum()\n",
    "   porcentaje_nulos = (total_feature_nulos / df.shape[0]) * 100\n",
    "   print(f'Total de valores nulos de {feature}: {total_feature_nulos} ({porcentaje_nulos:.2f}%)')\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Total de valores nulos en toda la data:\", total_nulos)\n",
    "print(\"Total de datos en toda la data:\", total_datos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5a5f55-6655-4833-bd89-da6fd0f6051a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0bmOJpN2m_a",
    "outputId": "cf80d209-720b-4d8d-9fb0-1e0f68699a63"
   },
   "outputs": [],
   "source": [
    "# Calcular el total de valores nulos en toda la data\n",
    "total_nulos = df.isnull().sum().sum()\n",
    "\n",
    "# Calcular el total de datos en toda la data\n",
    "total_datos = df.size\n",
    "\n",
    "# Función para identificar outliers utilizando el método del rango intercuartílico (IQR)\n",
    "def identificar_outliers(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"TOTAL DE OUTLIERS\")\n",
    "for feature in df.columns:\n",
    "    if df[feature].dtype in ['float64', 'int64']:  # Considerar solo columnas numéricas\n",
    "        total_feature_outliers = identificar_outliers(df[feature])\n",
    "        porcentaje_outliers = (total_feature_outliers / df.shape[0]) * 100\n",
    "        print(f'Total de outliers en {feature}: {total_feature_outliers} ({porcentaje_outliers:.2f}%)')\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Total de valores nulos en toda la data:\", total_nulos)\n",
    "print(\"Total de datos en toda la data:\", total_datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6098558e-be00-4421-82a4-9311913c5df7",
     "showTitle": false,
     "title": ""
    },
    "id": "-fZaMdUxgT5i"
   },
   "source": [
    "### Matriz de Correlación\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a75ab5-87c9-4815-a37b-6057a16ccafd",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "M3h61LHadlWi",
    "outputId": "fdde14e7-a5a3-4549-bd88-fbebc4f1dfae"
   },
   "outputs": [],
   "source": [
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm', 'Cloud9am', 'Cloud3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70357e5c-5ef4-4e9a-ae4f-835e607c0018",
     "showTitle": false,
     "title": ""
    },
    "id": "TdlV4hdnIYEs"
   },
   "source": [
    "### Variables Objetivo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49c473e-9828-499b-9e80-eafcfc7df987",
     "showTitle": false,
     "title": ""
    },
    "id": "5yJ_9cIJLnd3"
   },
   "source": [
    "#### Variable Objetivo Categórica: \"\"RainTomorrow\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f58b92-fbcb-4d20-9222-a2ef15361276",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5J1xB05Lrog",
    "outputId": "1d883749-bf4f-47ec-8a3a-e4dac7616151"
   },
   "outputs": [],
   "source": [
    "df[\"RainTomorrow\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2171f6f-309d-4562-b6d1-0ab1513bfbb4",
     "showTitle": false,
     "title": ""
    },
    "id": "USyHjzGFz_kw"
   },
   "source": [
    "#### Variable Objetivo Numérica: \"\"RISK_MM\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edff38eb-d36c-4640-a2a2-b8cb0c0a8940",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MS_IqvUk0eRN",
    "outputId": "7445782b-728e-4d75-de3f-116e01d03d9f"
   },
   "outputs": [],
   "source": [
    "df[\"RISK_MM\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49c2cce-0149-491f-8ab4-6000d4706208",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "HkxaMURr0ULR",
    "outputId": "f9beed43-09a6-4275-9e0e-4fcfb0ae7363"
   },
   "outputs": [],
   "source": [
    "# Obtener las variables numéricas\n",
    "numericas = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "corr = numericas.corr()\n",
    "\n",
    "# Seleccionar las correlaciones más interesantes\n",
    "corr_seleccionadas = corr[corr['RISK_MM'].abs() > 0.5]\n",
    "\n",
    "# Crear un mapa de calor de la matriz de correlación\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_seleccionadas, cmap='coolwarm', annot=True)\n",
    "plt.title('Mapa de calor de las correlaciones más interesantes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9d8606-4a23-4a8a-b9e1-c93eab30e813",
     "showTitle": false,
     "title": ""
    },
    "id": "HhB6lSZAK-f_"
   },
   "source": [
    "### **Hallazgo 1: Falta de datos**\n",
    "---\n",
    "\n",
    "El gráfico muestra la cantidad de filas de datos por año. Se observa que los años 2007, 2008, 2017 y 2018 presentan una cantidad significativamente menor de datos en comparación con los demás años.\n",
    "\n",
    "Un conjunto de datos con un número reducido de observaciones puede no ser representativo de la población general, lo que puede llevar a conclusiones erróneas o poco precisas. Por lo mismo se opto por eliminar dichos años para mejorar la precisión del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad841ce-4a05-4713-97ac-2304d594d556",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "4dHdMJcnK9DX",
    "outputId": "06436b8b-82ae-48d5-a4f2-f3965c7079c5"
   },
   "outputs": [],
   "source": [
    "# Convertir la columna 'Date' a formato de fecha\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Obtener el año de cada fila\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Contar la cantidad de filas por año\n",
    "rows_by_year = df['Year'].value_counts().sort_index()\n",
    "\n",
    "# Crear un índice de todos los años presentes en los datos\n",
    "all_years = range(df['Year'].min(), df['Year'].max() + 1)\n",
    "\n",
    "# Rellenar los años faltantes con ceros\n",
    "rows_by_year = rows_by_year.reindex(all_years, fill_value=0)\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(rows_by_year.index, rows_by_year, color='skyblue')\n",
    "\n",
    "# Agregar el número de filas encima de cada barra\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), va='bottom')\n",
    "\n",
    "plt.title('Cantidad de filas por año')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Cantidad de filas')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99b6bc5-2e40-4e03-a890-d07dd4b80c52",
     "showTitle": false,
     "title": ""
    },
    "id": "0b-RnzXxBIaN"
   },
   "source": [
    "### **Hallazgo 2: La niña**\n",
    "---\n",
    "**Explicación:**\n",
    "\n",
    "El fenómeno de la Niña es cuando el agua en el océano Pacífico cerca del ecuador se enfría más de lo normal, lo que puede provocar cambios en el clima, como más lluvias en algunas áreas, sequías en otras y más tormentas tropicales.\n",
    "\n",
    "Este fenómeno se dio en Australia durante los años 2010 y 2011, el cual afecto mayormente al area de Queensland, provocando una de las peores inundaciones de la ultima decada en este país.\n",
    "\n",
    "*   **Inundaciones de Queensland:** Queensland experimentó una de las peores inundaciones en su historia debido a lluvias torrenciales. Esta inundación afectó gravemente la minería, especialmente las minas de carbón, ya que muchas quedaron inundadas y hubo interrupciones significativas en la producción, lo que causo perdidas aproximadas de $4 billones .\n",
    "\n",
    "*   **Ciclón Yasi:** Este ciclón, uno de los más poderosos en la historia de Australia, golpeó Queensland en febrero de 2011. Causó daños considerables en las infraestructuras mineras y en la cadena de suministro.\n",
    "\n",
    "\n",
    "**Justificación:**\n",
    "El fenómeno se respalda en nuestros datos debido a que, como se muestra en el gráfico, existe una gran cantidad de lluvia en esos años a diferencia de los otros.\n",
    "\n",
    "\n",
    "\n",
    "**Fuentes:**\n",
    "\n",
    "[MDPI](https://www.mdpi.com/2073-4441/3/4/1149)\n",
    "\n",
    "[Geoscience Australia](https://www.ga.gov.au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e5dde0-3ea6-415b-a507-4f6b1af51eac",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "xJwA40CZ_JXl",
    "outputId": "51d4f233-bc2e-47b4-8446-4a3fcfbd9aae"
   },
   "outputs": [],
   "source": [
    "# Convertir la columna 'Date' a tipo datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Filtrar los datos para incluir solo los años 2009 a 2015\n",
    "df_filtered = df[(df['Date'].dt.year >= 2009) & (df['Date'].dt.year <= 2015)]\n",
    "\n",
    "# Sumar la cantidad de lluvia por año\n",
    "rain_by_year = df_filtered.groupby(df_filtered['Date'].dt.year)['RISK_MM'].sum().reset_index()\n",
    "\n",
    "# Ordenar los años de mayor a menor cantidad de lluvia (y mantener ese orden)\n",
    "rain_by_year_sorted = rain_by_year.sort_values(by='RISK_MM', ascending=False)\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(rain_by_year_sorted)), rain_by_year_sorted['RISK_MM'], color='blue')  # Usamos range(len()) para asignar posiciones\n",
    "plt.xlabel('Año (ordenado por cantidad de lluvia)')\n",
    "plt.ylabel('Cantidad de lluvia (mm)')\n",
    "plt.title('Cantidad de lluvia por año (2009-2015)')\n",
    "plt.xticks(range(len(rain_by_year_sorted)), rain_by_year_sorted['Date'])  # Asignamos etiquetas según el nuevo orden\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3371e6a2-b3c6-4ed7-b756-1beb6c64f162",
     "showTitle": false,
     "title": ""
    },
    "id": "H7yCdJRBtTY8"
   },
   "source": [
    "### **Hallazgo 3: Sequías significas entre el año 2013 y 2015**\n",
    "---\n",
    "\n",
    "El gráfico muestra la cantidad promedio anual de lluvia en Australia entre 2009 y 2015. Se observa una tendencia a la baja en la precipitación durante este período, con un mínimo en el año 2014.\n",
    "\n",
    "De acuerdo a información real, Australia experimentó una sequía severa entre 2009 y 2015. Esta sequía fue la peor en la historia registrada del país y afectó a una gran parte del territorio.\n",
    "\n",
    "La sequía tuvo un impacto significativo en la industria minera de Australia, de la siguiente manera:\n",
    "\n",
    "* **Reducción de la producción:** La falta de agua afectó la disponibilidad de agua para los procesos de minería, lo que llevó a una reducción en la producción de minerales.\n",
    "* **Aumento de los costos:** La sequía también provocó un aumento en los costos de la minería, ya que las empresas tuvieron que invertir más en la búsqueda de agua y en el transporte de agua a los sitios mineros.\n",
    "* **Impactos ambientales:** La sequía también tuvo un impacto negativo en el medio ambiente, ya que la minería requiere grandes cantidades de agua.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4119ce0a-c238-4410-9e44-8adf26e1a2c5",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "lEIO66uVKahm",
    "outputId": "ea4a5f24-8949-4cb8-c1f3-e71a8e2b0dd2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convertir la columna 'Date' a tipo datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Filtrar los datos para incluir solo los años 2009 a 2015\n",
    "df_filtered = df[(df['Date'].dt.year >= 2009) & (df['Date'].dt.year <= 2015)]\n",
    "\n",
    "# Calcular la cantidad de lluvia promedio por año\n",
    "rain_by_year = df_filtered.groupby(df_filtered['Date'].dt.year)['Rainfall'].mean().reset_index()\n",
    "rain_by_year = df_filtered.groupby(df_filtered['Date'].dt.year)['Rainfall'].mean().reset_index()\n",
    "# Crear el gráfico de líneas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rain_by_year['Date'], rain_by_year['Rainfall'], marker='o', color='b', linestyle='-')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "plt.title('Rainfall promedio por año (2009-2015)')\n",
    "plt.grid(True)\n",
    "plt.xticks(rain_by_year['Date'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9542e3ab-5e76-47a0-8786-47e2aa6678ed",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "Mb-UWkmbXZYJ",
    "outputId": "c733894d-a313-4500-a4b9-0966db81acd0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convertir la columna 'Date' a formato datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Filtrar datos para los años de interés\n",
    "years_group1 = [2014]\n",
    "years_group2 = [2011]\n",
    "\n",
    "df_group1 = df[df['Date'].dt.year.isin(years_group1)]\n",
    "df_group2 = df[df['Date'].dt.year.isin(years_group2)]\n",
    "\n",
    "nulos_2014 = df_group1.isnull().sum().sum()\n",
    "nulos_2011 = df_group2 .isnull().sum().sum()\n",
    "\n",
    "# Definir columnas de interés para el análisis (puedes ajustar según necesites)\n",
    "columns_of_interest = ['MinTemp', 'MaxTemp', 'Rainfall','RISK_MM',  'Sunshine', 'WindGustSpeed', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm']\n",
    "\n",
    "# Crear matrices de correlación para cada grupo\n",
    "correlation_matrix_group1 = df_group1[columns_of_interest].corr()\n",
    "correlation_matrix_group2 = df_group2[columns_of_interest].corr()\n",
    "\n",
    "# Configurar el tamaño del gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(correlation_matrix_group1, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(f'Matriz de Correlación 2014\\nValores nulos: {nulos_2014}')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Mapa de calor para el grupo 2 (2010, 2011)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(correlation_matrix_group2, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(f'Matriz de Correlación 2011\\nValores nulos: {nulos_2011}')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Ajustar el diseño del subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar los mapas de calor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9f0c36-dae9-4510-a78e-ed3280772242",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n-NAymSzFtz0",
    "outputId": "de2d0b1a-f3de-44ee-ff2b-93c657f7e575"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Supongamos que ya tienes df_filtered, rain_by_year y wind_speed_by_year definidos como en tu código\n",
    "\n",
    "# Calcular la presión promedio por año\n",
    "pressure_by_year = df_filtered.groupby(df_filtered['Date'].dt.year)['Pressure9am'].mean().reset_index()\n",
    "wind_speed_by_year = df_filtered.groupby(df_filtered['Date'].dt.year)['WindGustSpeed'].mean().reset_index()\n",
    "\n",
    "# Crear una figura con tres subplots (uno encima del otro)\n",
    "fig, ( ax2, ax3) = plt.subplots(nrows=2, ncols=1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "\n",
    "# Gráfico para promedio de velocidad del viento por año\n",
    "ax2.plot(wind_speed_by_year['Date'], wind_speed_by_year['WindGustSpeed'], marker='s', color='g', linestyle='-', label='WindGustSpeed')\n",
    "ax2.set_ylabel('Promedio de velocidad del viento')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gráfico para promedio de presión a las 9 am por año\n",
    "ax3.plot(pressure_by_year['Date'], pressure_by_year['Pressure9am'], marker='^', color='r', linestyle='-', label='Pressure9am')\n",
    "ax3.set_xlabel('Año')\n",
    "ax3.set_ylabel('Promedio de presión (9 am)')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Ajustar el diseño de los subplots para evitar superposiciones\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17581aea-ce7c-465d-86c5-7c0c409177fa",
     "showTitle": false,
     "title": ""
    },
    "id": "ZNYKq9ITM4lI"
   },
   "source": [
    "## 3.- Preparación de los datos.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b47b15-c7d8-4db5-bf63-8058084c71b6",
     "showTitle": false,
     "title": ""
    },
    "id": "LXA2DoI_VPL0"
   },
   "source": [
    "Filtrar fechas para comprensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b9c3b0-43b2-463b-a2d1-0e5bfa4baf30",
     "showTitle": false,
     "title": ""
    },
    "id": "xtWe6gTJNZBB"
   },
   "outputs": [],
   "source": [
    "# Convertir la columna 'Date' a formato DateTime\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Filtrar el DataFrame para excluir los años 2007, 2008, 2017 y 2018\n",
    "df = df[(df['Date'].dt.year >= 2009) & (df['Date'].dt.year <= 2015)]  # Filtrar por año\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8097ef59-1f39-410d-a265-0a5f6a75eaad",
     "showTitle": false,
     "title": ""
    },
    "id": "kbWxhpE-CrsG"
   },
   "source": [
    "Filtrar por ubicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abd53ac-c5b1-43f8-a2ac-3cf8e0df7592",
     "showTitle": false,
     "title": ""
    },
    "id": "Ojke0kXTCpzl"
   },
   "outputs": [],
   "source": [
    "# Definir las ubicaciones para cada región\n",
    "new_south_wales = ('Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree', 'Newcastle', 'NorahHead', 'Penrith', 'Richmond', 'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown', 'Wollongong', 'NorfolkIsland')\n",
    "\n",
    "# Crear datasets para cada región\n",
    "df = df[df['Location'].isin(new_south_wales)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffbc300-4660-459a-b703-80cdb48657fd",
     "showTitle": false,
     "title": ""
    },
    "id": "ZXZh0H62yCqI"
   },
   "source": [
    "###Eliminar Nulos que esten por sobre el 37%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f67350f-f329-4f14-a30c-8726a061612e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4ex55loyVu_",
    "outputId": "94604561-ebe4-40df-98ed-fd5ad3583547"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calcular el total de valores nulos en toda la data\n",
    "total_nulos = df.isnull().sum().sum()\n",
    "\n",
    "# Calcular el total de datos en toda la data\n",
    "total_datos = df.size\n",
    "\n",
    "# Identificar las columnas que tienen un porcentaje de nulos mayor al 37%\n",
    "threshold = 36\n",
    "columns_to_drop = [feature for feature in df.columns if (df[feature].isna().sum() / df.shape[0]) * 100 > threshold]\n",
    "\n",
    "# Eliminar las columnas identificadas del DataFrame original\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"TOTAL DE VALORES NULOS\")\n",
    "for feature in df.columns:\n",
    "    total_feature_nulos = df[feature].isna().sum()\n",
    "    porcentaje_nulos = (total_feature_nulos / df.shape[0]) * 100\n",
    "    print(f'Total de valores nulos de {feature}: {total_feature_nulos} ({porcentaje_nulos:.2f}%)')\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Total de valores nulos en toda la data (después de eliminar columnas):\", df.isnull().sum().sum())\n",
    "print(\"Total de datos en toda la data (después de eliminar columnas):\", df.size)\n",
    "print(\"Columnas eliminadas:\", columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508c2917-39a6-4cf8-a944-9800a7f72fa0",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "DZBEq8x_yrdz",
    "outputId": "666ecf6a-a77f-4efa-a3a7-91641addc342"
   },
   "outputs": [],
   "source": [
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ee5db8-d41d-4557-b96b-78523eac3fd5",
     "showTitle": false,
     "title": ""
    },
    "id": "oF210l2Dxboz"
   },
   "source": [
    "### Inputación de variables categoricas\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a263b9-395d-4b1f-b782-d406d2670902",
     "showTitle": false,
     "title": ""
    },
    "id": "51wVKEtUC37-"
   },
   "source": [
    "Las variables categóricas, tales como **'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', y 'RainTomorrow'**, requieren una imputación de valores nulos que mantenga la coherencia con la naturaleza de los datos. Dado que estas variables son nominales y representan categorías discretas, la imputación por **moda** es la elección adecuada. La **moda** representa el valor más frecuente en un conjunto de datos, lo que garantiza que la imputación preserve la distribución original de las categorías.\n",
    "\n",
    "La imputación por **moda** se justifica porque preserva la distribución original de las categorías, asegurando que los valores imputados reflejen la tendencia predominante en los datos observados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8abfdd-4b11-484f-a881-e085aea2534d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd7LCE_UD1Ps",
    "outputId": "532fff1b-d30b-485d-bbcc-7c0f1a1b0e06"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imputación de valores nulos para las variables categóricas por moda\n",
    "categorical_variables = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n",
    "for var in categorical_variables:\n",
    "    df[var].fillna(df[var].mode()[0], inplace=True)\n",
    "\n",
    "# Verificar la cantidad de valores nulos después de la imputación\n",
    "print(\"Valores nulos después de la imputación:\")\n",
    "print(df[['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c05d795-1989-4374-9f95-a2772e9cb02d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "vRSOz7D7Wxvr",
    "outputId": "31dda94e-6d67-44a4-e8e3-804cab312531"
   },
   "outputs": [],
   "source": [
    "#Matriz despues de haber imputado los nulos de variables categoricas\n",
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación sin nulos catégoricos', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9888a66b-4a4a-4624-8021-d193c25ba2f4",
     "showTitle": false,
     "title": ""
    },
    "id": "uK0spYIE1JcB"
   },
   "source": [
    "### Imputación de Variables Numericas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19636d86-c70c-4632-88ef-660203738c30",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KgAQoE-y4yfT",
    "outputId": "3a099db0-57c3-4350-cbbf-a76850875652"
   },
   "outputs": [],
   "source": [
    "# Definir las columnas numéricas\n",
    "numerical_cols = [\n",
    "    'MinTemp', 'MaxTemp', 'Rainfall',\n",
    "    'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "    'Humidity3pm', 'Pressure9am', 'Pressure3pm',\n",
    "    'Temp9am', 'Temp3pm', 'RISK_MM'\n",
    "]\n",
    "\n",
    "# Visualizar distribuciones de variables\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(5, 4, i)\n",
    "    sns.histplot(df[col].dropna(), kde=True)\n",
    "    plt.title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e040bc-af77-442f-b91c-f77cbd3f7585",
     "showTitle": false,
     "title": ""
    },
    "id": "_c1BYRpoAIFt"
   },
   "source": [
    "\n",
    "Para realizar la imputacion de los valores nulos para las variables numericas, se realizo un analisis mediante histogramas para visualizar la distribución de cada variable numérica en el dataset, con el cual se permite tomar una desicion para imputar segun corresponda a la distibución de cada variable:\n",
    "\n",
    " **1. Variables con Distribución Aproximadamente Normal**\n",
    "\n",
    "**Variables: `MinTemp`, `MaxTemp`, `Temp9am`, `Temp3pm`, `RISK_MM`, `Rainfall`**\n",
    "\n",
    "Estas variables presentan distribuciones aproximadamente normales. En estos casos, la **media** es una buena opción para la imputación de valores faltantes, ya que es representativa del centro de la distribución y permite mantener la simetría de los datos.\n",
    "\n",
    "**2. Variables con Distribución Sesgada a la Derecha**\n",
    "\n",
    "**Variables:  `Evaporation`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`**\n",
    "\n",
    "Estas variables presentan distribuciones sesgadas a la derecha, con muchos valores cercanos a cero. En estos casos, la **mediana** es preferible para la imputación, ya que es menos sensible a los valores extremos y proporciona una medida más robusta del centro de la distribución.\n",
    "\n",
    "**3. Variables con Distribuciones Sesgadas y Difíciles de Categorizar**\n",
    "\n",
    "**Variables: `Sunshine`, `Humidity9am`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`, `Cloud9am`, `Cloud3pm`**\n",
    "\n",
    "Para estas variables, aunque presentan distribuciones sesgadas, la imputación por **mediana** es generalmente más adecuada debido a su robustez ante valores atípicos. Dado que la **mediana** no se ve afectada por extremos, es una opción segura para mantener la integridad de los datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "effa7e90-0def-45da-8feb-ac780a9a8627",
     "showTitle": false,
     "title": ""
    },
    "id": "iYPblpyCZIPc"
   },
   "outputs": [],
   "source": [
    "# Imputación de valores nulos para variables con distribución normal\n",
    "normal_variables = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'RISK_MM', 'Rainfall']\n",
    "for var in normal_variables:\n",
    "    df[var].fillna(df[var].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e8d29c-5d8d-4053-9526-7e8c3547b321",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "n0jVwBKhZRPM",
    "outputId": "8382934c-cf54-4b92-e5bf-316014fc98af"
   },
   "outputs": [],
   "source": [
    "#Matriz despues de haber imputado por la media\n",
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538c4df0-acb7-45f3-9957-cc353f487265",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhM8o-dvA09Y",
    "outputId": "9a35a880-57d3-4833-ff14-ea94a6e3dab2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imputación de valores nulos para variables con distribución sesgada a la derecha\n",
    "skewed_variables = ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm']\n",
    "for var in skewed_variables:\n",
    "    df[var].fillna(df[var].median(), inplace=True)\n",
    "\n",
    "# Imputación de valores nulos para variables con distribuciones sesgadas y difíciles de categorizar\n",
    "difficult_variables = ['Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm']\n",
    "for var in difficult_variables:\n",
    "    df[var].fillna(df[var].median(), inplace=True)\n",
    "\n",
    "\n",
    "# Verificar la cantidad de valores nulos después de la imputación\n",
    "print(\"Valores nulos después de la imputación:\")\n",
    "print(df[['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm',\n",
    "          'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n",
    "          'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3073fe9d-22c8-4220-95fd-2c3ffff43b85",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "S149fmo6W-WG",
    "outputId": "139e0146-8bd6-425d-f64b-618f7f0fad8a"
   },
   "outputs": [],
   "source": [
    "#Matriz despues de haber imputado por la mediana\n",
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9477ed1-8440-4972-a088-a680d42184d6",
     "showTitle": false,
     "title": ""
    },
    "id": "i0hXlzad5DBs"
   },
   "source": [
    "###Imputación de Outlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c2df28-5d59-4918-b04a-89545c2f1b8e",
     "showTitle": false,
     "title": ""
    },
    "id": "ejKOYq9x5Ctf"
   },
   "outputs": [],
   "source": [
    "def treat_outliers_with_mean(df):\n",
    "    variables_interes = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm']\n",
    "    for variable in variables_interes:\n",
    "        if df[variable].dtype != 'object':  # Solo trabajamos con columnas numéricas\n",
    "            Q1 = df[variable].quantile(0.25)\n",
    "            Q3 = df[variable].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Determinar los límites inferior y superior\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Identificar outliers\n",
    "            outliers_lower = df[df[variable] < lower_bound]\n",
    "            outliers_upper = df[df[variable] > upper_bound]\n",
    "\n",
    "            # Calcular la media de la columna sin considerar los outliers\n",
    "            mean_value = df[(df[variable] >= lower_bound) & (df[variable] <= upper_bound)][variable].mean()\n",
    "\n",
    "            # Reemplazar outliers por la media\n",
    "            df.loc[outliers_lower.index, variable] = mean_value\n",
    "            df.loc[outliers_upper.index, variable] = mean_value\n",
    "\n",
    "\n",
    "\n",
    "treat_outliers_with_mean(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73684e00-7248-4b88-aefa-3e226ac0c8e6",
     "showTitle": false,
     "title": ""
    },
    "id": "EBKBeKXN_HEu"
   },
   "outputs": [],
   "source": [
    "def treat_outliers_with_median(df):\n",
    "    variables_interes = ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm']\n",
    "    for variable in variables_interes:\n",
    "        if df[variable].dtype != 'object':  # Solo trabajamos con columnas numéricas\n",
    "            Q1 = df[variable].quantile(0.25)\n",
    "            Q3 = df[variable].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Determinar los límites inferior y superior\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Identificar outliers\n",
    "            outliers_lower = df[df[variable] < lower_bound]\n",
    "            outliers_upper = df[df[variable] > upper_bound]\n",
    "\n",
    "            # Calcular la media de la columna sin considerar los outliers\n",
    "            median_value = df[(df[variable] >= lower_bound) & (df[variable] <= upper_bound)][variable].median()\n",
    "\n",
    "            # Reemplazar outliers por la media\n",
    "            df.loc[outliers_lower.index, variable] = median_value\n",
    "            df.loc[outliers_upper.index, variable] = median_value\n",
    "\n",
    "\n",
    "\n",
    "treat_outliers_with_median(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0b04a4-8ac5-45fe-a695-6e8d3efbd9e3",
     "showTitle": false,
     "title": ""
    },
    "id": "UxjwI5_uA3R8"
   },
   "outputs": [],
   "source": [
    "def treat_outliers(df):\n",
    "    variables_interes = [ 'Rainfall','RISK_MM']\n",
    "    for variable in variables_interes:\n",
    "        if df[variable].dtype != 'object':  # Solo trabajamos con columnas numéricas\n",
    "            Q1 = df[variable].quantile(0.25)\n",
    "            Q3 = df[variable].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Determinar los límites inferior y superior\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Identificar outliers\n",
    "            outliers_lower = df[df[variable] < lower_bound]\n",
    "            outliers_upper = df[df[variable] > upper_bound]\n",
    "\n",
    "            # Reemplazar outliers por el valor correspondiente al límite inferior o superior\n",
    "            df.loc[outliers_lower.index, variable] = lower_bound\n",
    "            df.loc[outliers_upper.index, variable] = upper_bound\n",
    "\n",
    "#Llamar a la función para tratar outliers\n",
    "treat_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e94aaeea-92cd-4447-951a-4a3925aa912f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fPQz3uFDQI6",
    "outputId": "d9fe65bf-957b-4d1c-c2c2-6f8d36b929f8"
   },
   "outputs": [],
   "source": [
    "def print_outliers(df):\n",
    "    print(\"\\nTOTAL DE OUTLIERS\")\n",
    "    for variable in df:\n",
    "        if df[variable].dtype != 'object':  # Solo trabajamos con columnas numéricas\n",
    "            Q1 = df[variable].quantile(0.25)\n",
    "            Q3 = df[variable].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            outliers = df[(df[variable] < lower_bound) | (df[variable] > upper_bound)]\n",
    "            total_outliers = len(outliers)\n",
    "            print(f\"Total de outliers de {variable}: {total_outliers}\")\n",
    "\n",
    "# Llamamos a la función para imprimir los outliers\n",
    "print_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8222734d-4782-4d72-bc40-f4559a63681b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "WuVDEsyHAIqa",
    "outputId": "0ccb6f08-55b5-41c3-f9d8-3679e8677c9b"
   },
   "outputs": [],
   "source": [
    "#Matriz despues de haber imputado los Outlaiers\n",
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67a004bc-fced-4478-8d9d-47767e8eb0ce",
     "showTitle": false,
     "title": ""
    },
    "id": "h5Nps8cA2tpm"
   },
   "source": [
    "###**<h1>Ingeniería de características</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae6d66c-19c1-4006-b4cd-c1a5d6a4c26b",
     "showTitle": false,
     "title": ""
    },
    "id": "9pYxjeOX21_3"
   },
   "source": [
    "**DATE** <BR>\n",
    "Es importante desglosar la fecha en diferentes columnas (día, mes, año) porque nos permite entender mejor cómo cambian las cosas con el tiempo.\n",
    "\n",
    "Al dividir la fecha en partes más pequeñas y poner cada parte en su propia columna, es más fácil ver estos patrones. Por ejemplo, podrías mirar cuánta lluvia cae en promedio en cada mes o cómo han cambiado las temperaturas cada año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70622e86-b3f4-45d3-b325-89f2f50790c9",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CwqXV9e22ov",
    "outputId": "40145c1f-c066-4cd1-a07b-367500223ecd"
   },
   "outputs": [],
   "source": [
    "# Convertir la columna \"Date\" a formato de fecha\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Crear nuevas columnas para día, mes y año\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Imprimir las primeras filas para verificar\n",
    "print(df[['Date', 'Day', 'Month', 'Year']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59491b6-680b-4ed2-bf69-3bcf0cdd090e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqM2d_Ft27LY",
    "outputId": "dfce2c0c-c237-4cc3-abaa-bce8ce92ed03"
   },
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Función para obtener la estación del año en el\n",
    "#hemisferio sur (Australia)\n",
    "def obtener_estacion_australia(fecha):\n",
    "    mes = fecha.month\n",
    "    if mes in (12, 1, 2):\n",
    "        return 'Verano'\n",
    "    elif mes in (3, 4, 5):\n",
    "        return 'Otoño'\n",
    "    elif mes in (6, 7, 8):\n",
    "        return 'Invierno'\n",
    "    else:\n",
    "        return 'Primavera'\n",
    "\n",
    "# Aplicamos la función a la columna 'Date' y guardamos el\n",
    "#resultado en una nueva columna llamada 'Season'\n",
    "df['Season'] = df['Date'].apply(obtener_estacion_australia)\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df[\"Season\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aded6a61-0009-40af-948e-82d494a0d621",
     "showTitle": false,
     "title": ""
    },
    "id": "PR9uznkY3EQS"
   },
   "source": [
    "Despues de hacer esta ingeniería de caracteristicas, borraremos la variable Date, ya que no la necesitaremos al tener las variables Day, Month, Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0f8861-22cd-4d5f-87cb-52bc472f7824",
     "showTitle": false,
     "title": ""
    },
    "id": "CDb_ML8u3Eyq"
   },
   "outputs": [],
   "source": [
    "# Eliminar la variable 'variable_a_eliminar' del DataFrame df\n",
    "df.drop(columns=['Date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee916a07-cfec-4b72-a3b6-b8338ee9b58a",
     "showTitle": false,
     "title": ""
    },
    "id": "4TC-_wvay8V_"
   },
   "source": [
    "### **LaberEncoder()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb3144a-929b-49ac-a601-26f15f780ab9",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UY4V-_7y3In",
    "outputId": "76441b97-ad86-4a8b-e549-30e0e94c7fc2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Lista de variables categóricas en tu DataFrame\n",
    "categorical_variables = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow', 'Season']\n",
    "\n",
    "# Creamos el codificador LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Aplicamos el LabelEncoder a cada variable categórica en el DataFrame\n",
    "for var in categorical_variables:\n",
    "    df[var] = encoder.fit_transform(df[var])\n",
    "\n",
    "\n",
    "print(df[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac4e6c9-76ed-45e1-aac5-d1937800b269",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "GZBsOTW31rn7",
    "outputId": "52c1c44a-826f-4e8b-83e7-a1bf96344380"
   },
   "outputs": [],
   "source": [
    "#Matriz despues de haber realizado LabelEncoder()\n",
    "#Seleccionar las variables no categóricas\n",
    "variables_no_categoricas = ['RISK_MM', 'Humidity3pm',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Pressure9am', 'Pressure3pm','Rainfall']\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las variables no categóricas\n",
    "df_no_categoricas = df[variables_no_categoricas]\n",
    "\n",
    "# Calcular la matriz de correlación para las variables no categóricas\n",
    "corr_matrix_no_categoricas = df_no_categoricas.corr()\n",
    "\n",
    "# Crear una figura de tamaño adecuado\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Generar el mapa de calor de la matriz de correlación para las variables no categóricas\n",
    "sns.heatmap(corr_matrix_no_categoricas, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c50eb25-0220-48f7-9682-b39e6b92730d",
     "showTitle": false,
     "title": ""
    },
    "id": "Plofn04rn4yM"
   },
   "source": [
    "## 4.- Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbfd3de-b64f-463d-8f63-374bdc488d40",
     "showTitle": false,
     "title": ""
    },
    "id": "BrdgFiAayG-f"
   },
   "outputs": [],
   "source": [
    "#Importar librerias\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb2304c-8e04-48b6-a228-744c535e05fd",
     "showTitle": false,
     "title": ""
    },
    "id": "AcGQjeUJLhSJ"
   },
   "source": [
    "## Modelos de RISK_MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e7da1b-75ac-4abe-b302-31adb79c95c1",
     "showTitle": false,
     "title": ""
    },
    "id": "raM4sZVSykxG"
   },
   "outputs": [],
   "source": [
    "X = df.drop('RISK_MM', axis=1)\n",
    "y = df['RISK_MM']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700cf3e4-eb12-4b67-8f69-3a8964839cdb",
     "showTitle": false,
     "title": ""
    },
    "id": "k70gwFcjyspg"
   },
   "source": [
    "###Regresion Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f0baeb-103a-46fa-a562-dc14cfac7795",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wL3bLjRyufH",
    "outputId": "aa0ff59c-59eb-47ce-b32e-23dad53799f3"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de regresión lineal\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_predictions = linear_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de regresión lineal\n",
    "linear_rmse = np.sqrt(mean_squared_error(y_test, linear_predictions))\n",
    "linear_mse = mean_squared_error(y_test, linear_predictions)\n",
    "linear_mae = mean_absolute_error(y_test, linear_predictions)\n",
    "linear_r2 = r2_score(y_test, linear_predictions)\n",
    "\n",
    "# Imprimir métricas de regresión lineal\n",
    "print(f'Regresión Lineal Metrics:')\n",
    "print(f'RMSE: {linear_rmse}')\n",
    "print(f'MSE: {linear_mse}')\n",
    "print(f'MAE: {linear_mae}')\n",
    "print(f'R^2 Score: {linear_r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e4679c-a1d3-4ea2-922f-f52c0ba9820b",
     "showTitle": false,
     "title": ""
    },
    "id": "zQx6P4XEz5_B"
   },
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee39c4f-fcf2-4438-9685-c5808077916b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U-ZEOu6z6yA",
    "outputId": "1d7e4fdb-cb8c-48a0-d307-3f854e89f673"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de KNN para regresión\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de KNN\n",
    "knn_rmse = np.sqrt(mean_squared_error(y_test, knn_predictions))\n",
    "knn_mse = mean_squared_error(y_test, knn_predictions)\n",
    "knn_mae = mean_absolute_error(y_test, knn_predictions)\n",
    "knn_r2 = r2_score(y_test, knn_predictions)\n",
    "\n",
    "# Imprimir métricas de KNN\n",
    "print(f'\\nKNN Metrics:')\n",
    "print(f'RMSE: {knn_rmse}')\n",
    "print(f'MSE: {knn_mse}')\n",
    "print(f'MAE: {knn_mae}')\n",
    "print(f'R^2 Score: {knn_r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73eaf19-cba1-4d94-bd30-a2ffa8b7aa94",
     "showTitle": false,
     "title": ""
    },
    "id": "xOU62oUQ0PTX"
   },
   "source": [
    "###RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5adc88-26cb-400c-a865-73df2e0dc89b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dl3zRn860R_n",
    "outputId": "bb455613-6289-4cc2-817e-1845f9fdf8d6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Crear y entrenar el modelo de bosques aleatorios para regresión\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(X_train, y_train)\n",
    "forest_predictions = forest_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de bosques aleatorios\n",
    "forest_rmse = np.sqrt(mean_squared_error(y_test, forest_predictions))\n",
    "forest_mse = mean_squared_error(y_test, forest_predictions)\n",
    "forest_mae = mean_absolute_error(y_test, forest_predictions)\n",
    "forest_r2 = r2_score(y_test, forest_predictions)\n",
    "\n",
    "# Imprimir métricas de bosques aleatorios\n",
    "print(f'\\nBosques Aleatorios Metrics:')\n",
    "print(f'RMSE: {forest_rmse}')\n",
    "print(f'MSE: {forest_mse}')\n",
    "print(f'MAE: {forest_mae}')\n",
    "print(f'R^2 Score: {forest_r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074eca52-59d6-442b-9c68-64e10b2d83bc",
     "showTitle": false,
     "title": ""
    },
    "id": "eIi5YUAROiet"
   },
   "source": [
    "####<h2>Métricas de Regresión</h2>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Métrica</th>\n",
    "        <th>Descripción</th>\n",
    "        <th>Rango</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RMSE (Error Cuadrático Medio)</td>\n",
    "        <td>Mide cuánto se equivoca nuestro modelo en sus predicciones, considerando tanto las predicciones demasiado altas como las demasiado bajas. Cuanto más bajo sea, mejor.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Menor que 1: Excelente</li>\n",
    "                <li>Entre 1 y 5: Muy bueno</li>\n",
    "                <li>Entre 5 y 10: Bueno</li>\n",
    "                <li>Mayor que 10: Aceptable</li>\n",
    "                <li>Mayor que 20: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MSE (Error Cuadrático Medio)</td>\n",
    "        <td>Es similar al RMSE, pero no tiene la raíz cuadrada. Mide el promedio de los errores al cuadrado. Cuanto más bajo, mejor.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Menor que 1: Excelente</li>\n",
    "                <li>Entre 1 y 25: Muy bueno</li>\n",
    "                <li>Entre 25 y 100: Bueno</li>\n",
    "                <li>Mayor que 100: Aceptable</li>\n",
    "                <li>Mayor que 400: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MAE (Error Absoluto Medio)</td>\n",
    "        <td>Mide cuánto se equivoca nuestro modelo en promedio, sin considerar la dirección de los errores. Cuanto más bajo sea, mejor.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Menor que 1: Excelente</li>\n",
    "                <li>Entre 1 y 5: Muy bueno</li>\n",
    "                <li>Entre 5 y 10: Bueno</li>\n",
    "                <li>Mayor que 10: Aceptable</li>\n",
    "                <li>Mayor que 20: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>R^2 Score (Coeficiente de Determinación)</td>\n",
    "        <td>Indica cuánta variación en los datos puede explicar nuestro modelo. Cuanto más cerca de 1, mejor.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Mayor que 0.9: Excelente</li>\n",
    "                <li>Entre 0.8 y 0.9: Muy bueno</li>\n",
    "                <li>Entre 0.7 y 0.8: Bueno</li>\n",
    "                <li>Entre 0.6 y 0.7: Aceptable</li>\n",
    "                <li>Menor que 0.6: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b0483f-e608-4f2f-a653-e46077cc2a21",
     "showTitle": false,
     "title": ""
    },
    "id": "gfONNvidLqsr"
   },
   "source": [
    "## Modelos de Rain_tomorrow\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7886d2-b93d-441e-b55b-2ff5a51652b5",
     "showTitle": false,
     "title": ""
    },
    "id": "aI4ekYUGPnCk"
   },
   "outputs": [],
   "source": [
    "#Importar librerias\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b24a74-5441-46d6-bb0a-4b2ed40973e3",
     "showTitle": false,
     "title": ""
    },
    "id": "HLhe3e7jN3cE"
   },
   "outputs": [],
   "source": [
    "X = df.drop('RainTomorrow', axis=1)\n",
    "y = df['RainTomorrow']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f04c78-d1bd-4c4c-83d2-df58eab30b9a",
     "showTitle": false,
     "title": ""
    },
    "id": "9cPoZpSJMAz4"
   },
   "source": [
    "### Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2f2987-e9dd-4efe-933a-0bf4c15d0d49",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lII7GMdmLwmi",
    "outputId": "91070872-2b5e-47d7-e1ce-f576868d7ac4"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de regresión logística\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "logistic_predictions = logistic_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de regresión logística\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "logistic_precision = precision_score(y_test, logistic_predictions)\n",
    "logistic_recall = recall_score(y_test, logistic_predictions)\n",
    "logistic_f1 = f1_score(y_test, logistic_predictions)\n",
    "\n",
    "# Imprimir métricas de regresión logística\n",
    "print(f'Regresión Logística Metrics:')\n",
    "print(f'Accuracy: {logistic_accuracy}')\n",
    "print(f'Precision: {logistic_precision}')\n",
    "print(f'Recall: {logistic_recall}')\n",
    "print(f'F1 Score: {logistic_f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aedcd3cf-9ef4-47d1-a7ac-767a606b2f38",
     "showTitle": false,
     "title": ""
    },
    "id": "YoIFYcnVThUi"
   },
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750fb3cf-b870-4e1f-b1b8-1f76e28a9cc1",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gh19poEDSKXU",
    "outputId": "26a4e71f-f4ce-4db2-d5dd-f28c0a41dfee"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de KNN para clasificación\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calcular métricas de KNN\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "knn_precision = precision_score(y_test, knn_predictions)\n",
    "knn_recall = recall_score(y_test, knn_predictions)\n",
    "knn_f1 = f1_score(y_test, knn_predictions)\n",
    "\n",
    "# Imprimir métricas de KNN\n",
    "print(f'\\nKNN Metrics:')\n",
    "print(f'Accuracy: {knn_accuracy}')\n",
    "print(f'Precision: {knn_precision}')\n",
    "print(f'Recall: {knn_recall}')\n",
    "print(f'F1 Score: {knn_f1}')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, knn_predictions)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e04c6a2a-1a0c-4731-9c0c-99382a6ab0eb",
     "showTitle": false,
     "title": ""
    },
    "id": "JZcPplCeTle8"
   },
   "source": [
    "### Super Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e77c7ad-dbf3-4350-a79f-7b1859faff1d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baQeFND8TPGQ",
    "outputId": "4985b425-e4af-4c3d-e139-1d2f5367b0f8"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Crear y entrenar el modelo SVM para clasificación\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de SVM\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "svm_precision = precision_score(y_test, svm_predictions)\n",
    "svm_recall = recall_score(y_test, svm_predictions)\n",
    "svm_f1 = f1_score(y_test, svm_predictions)\n",
    "\n",
    "# Imprimir métricas de SVM\n",
    "print(f'\\nSVM Metrics:')\n",
    "print(f'Accuracy: {svm_accuracy}')\n",
    "print(f'Precision: {svm_precision}')\n",
    "print(f'Recall: {svm_recall}')\n",
    "print(f'F1 Score: {svm_f1}')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, svm_predictions)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2451b0cd-ecf0-4cfd-8eeb-7dec220438de",
     "showTitle": false,
     "title": ""
    },
    "id": "KA1KjAhmR8ts"
   },
   "source": [
    "<h2>Métricas de Clasificación</h2>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Métrica</th>\n",
    "        <th>Descripción</th>\n",
    "        <th>Rango</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Accuracy (Exactitud)</td>\n",
    "        <td>Indica la proporción de predicciones correctas respecto al total. Cuanto más alto, mejor.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Mayor que 0.9: Excelente</li>\n",
    "                <li>Entre 0.8 y 0.9: Muy bueno</li>\n",
    "                <li>Entre 0.7 y 0.8: Bueno</li>\n",
    "                <li>Entre 0.6 y 0.7: Aceptable</li>\n",
    "                <li>Menor que 0.6: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Precision (Precisión)</td>\n",
    "        <td>Mide la proporción de verdaderos positivos sobre el total de predicciones positivas. Cuanto más alto, mejor. Es útil en contextos donde los falsos positivos son costosos.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Mayor que 0.9: Excelente</li>\n",
    "                <li>Entre 0.8 y 0.9: Muy bueno</li>\n",
    "                <li>Entre 0.7 y 0.8: Bueno</li>\n",
    "                <li>Entre 0.6 y 0.7: Aceptable</li>\n",
    "                <li>Menor que 0.6: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Recall (Sensibilidad o Tasa de Verdaderos Positivos)</td>\n",
    "        <td>Mide la proporción de verdaderos positivos sobre el total de verdaderos positivos y falsos negativos. Cuanto más alto, mejor. Es útil en contextos donde los falsos negativos son costosos.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Mayor que 0.9: Excelente</li>\n",
    "                <li>Entre 0.8 y 0.9: Muy bueno</li>\n",
    "                <li>Entre 0.7 y 0.8: Bueno</li>\n",
    "                <li>Entre 0.6 y 0.7: Aceptable</li>\n",
    "                <li>Menor que 0.6: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>F1 Score</td>\n",
    "        <td>Es la media armónica de precisión y recall. Proporciona una medida balanceada para modelos con clases desbalanceadas. Cuanto más alto, mejor.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>Mayor que 0.9: Excelente</li>\n",
    "                <li>Entre 0.8 y 0.9: Muy bueno</li>\n",
    "                <li>Entre 0.7 y 0.8: Bueno</li>\n",
    "                <li>Entre 0.6 y 0.7: Aceptable</li>\n",
    "                <li>Menor que 0.6: Cuestionable</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Confusion Matrix (Matriz de Confusión)</td>\n",
    "        <td>Proporciona una tabla con los valores de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos. No tiene un rango específico, pero debe ser interpretada para entender la distribución de errores.</td>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li>N/A: Debe ser interpretada en el contexto del problema específico.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3312884556580360,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "iteracion5Mineria",
   "widgets": {}
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
