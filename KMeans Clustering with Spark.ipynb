{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443a5963-89d0-4422-bdcc-4b265b19fb5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_data(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data: DataFrame = spark.read.csv(file_path, header=True, inferSchema=True, sep=';')\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_metrics(data_scaled: DataFrame, k_values: List[int]) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Calculate WCSS and silhouette score for different values of k.\n",
    "    \n",
    "    Args:\n",
    "    - data_scaled (DataFrame): DataFrame with scaled features.\n",
    "    - k_values (List[int]): List of k values for clustering.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[List[float], List[float]]: WCSS and silhouette scores.\n",
    "    \"\"\"\n",
    "    wcss: List[float] = []\n",
    "    silhouette_scores: List[float] = []\n",
    "    try:\n",
    "        for i in k_values:\n",
    "            kmeans: KMeans = KMeans(k=i, seed=0)\n",
    "            model = kmeans.fit(data_scaled)\n",
    "            summary = model.summary\n",
    "            wcss.append(summary.trainingCost)\n",
    "\n",
    "            # Calculate silhouette score\n",
    "            predictions = model.transform(data_scaled)\n",
    "            evaluator = ClusteringEvaluator()\n",
    "            silhouette_score: float = evaluator.evaluate(predictions)\n",
    "            silhouette_scores.append(silhouette_score)\n",
    "\n",
    "            print(f\"Para k={i}, WCSS={summary.trainingCost}, Puntuación de la silueta={silhouette_score}\")\n",
    "\n",
    "        return wcss, silhouette_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {str(e)}\")\n",
    "        return [], []\n",
    "\n",
    "def find_elbow_point(wcss: List[float]) -> int:\n",
    "    \"\"\"\n",
    "    Find the elbow point in the WCSS curve.\n",
    "    \n",
    "    Args:\n",
    "    - wcss (List[float]): List of WCSS values.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Optimal number of clusters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x1, y1 = 2, wcss[0]\n",
    "        x2, y2 = 16, wcss[-1]\n",
    "        distances: List[float] = []\n",
    "        for i in range(len(wcss)):\n",
    "            x0 = i + 2\n",
    "            y0 = wcss[i]\n",
    "            numerator = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2*y1 - y2*x1)\n",
    "            denominator = np.sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "            distances.append(numerator / denominator)\n",
    "        return distances.index(max(distances)) + 2\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding elbow point: {str(e)}\")\n",
    "        return -1\n",
    "\n",
    "def plot_metrics(k_values: List[int], wcss: List[float], silhouette_scores: List[float]) -> None:\n",
    "    \"\"\"\n",
    "    Plot WCSS and silhouette score.\n",
    "    \n",
    "    Args:\n",
    "    - k_values (List[int]): List of k values.\n",
    "    - wcss (List[float]): List of WCSS values.\n",
    "    - silhouette_scores (List[float]): List of silhouette scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wcss_rounded: List[float] = [round(value, 1) for value in wcss]\n",
    "        silhouette_scores_rounded: List[float] = [round(value, 2) for value in silhouette_scores]\n",
    "\n",
    "        plt.figure(figsize=(12, 12))\n",
    "\n",
    "        # Plot WCSS\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(k_values, wcss_rounded, marker='o')\n",
    "        plt.xlabel('Número de clusters (k)')\n",
    "        plt.ylabel('WCSS')\n",
    "        plt.title('WCSS para diferentes valores de k')\n",
    "        plt.xticks(np.arange(2, 17, step=1))\n",
    "        plt.yticks(np.round(np.linspace(min(wcss_rounded), max(wcss_rounded), 5), 1))\n",
    "\n",
    "        for i, txt in enumerate(wcss_rounded):\n",
    "            plt.text(k_values[i], wcss_rounded[i], str(txt), ha='center', va='bottom')\n",
    "\n",
    "        # Plot silhouette score\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(k_values, silhouette_scores_rounded, marker='o')\n",
    "        plt.xlabel('Número de clusters (k)')\n",
    "        plt.ylabel('Puntuación de la silueta')\n",
    "        plt.title('Puntuación de la silueta para diferentes valores de k')\n",
    "        plt.xticks(np.arange(2, 17, step=1))\n",
    "        plt.yticks(np.round(np.linspace(min(silhouette_scores_rounded), max(silhouette_scores_rounded), 5), 1))\n",
    "\n",
    "        for i, txt in enumerate(silhouette_scores_rounded):\n",
    "            plt.text(k_values[i], silhouette_scores_rounded[i], str(txt), ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting metrics: {str(e)}\")\n",
    "\n",
    "def main() -> None:\n",
    "    try:\n",
    "        # Load dataset\n",
    "        file_path: str = \"abfss://raw@cs2100320032141b0ad.dfs.core.windows.net/data_science/Libro1.csv\"\n",
    "        data: DataFrame = load_data(file_path)\n",
    "        if data is None:\n",
    "            return\n",
    "\n",
    "        # Assemble features\n",
    "        assembler: VectorAssembler = VectorAssembler(inputCols=data.columns[1:], outputCol=\"features\")\n",
    "        data_assembled: DataFrame = assembler.transform(data)\n",
    "\n",
    "        # Scale features\n",
    "        scaler: StandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "        scaler_model = scaler.fit(data_assembled)\n",
    "        data_scaled: DataFrame = scaler_model.transform(data_assembled)\n",
    "\n",
    "        # Calculate metrics\n",
    "        k_values: List[int] = list(range(2, 17))\n",
    "        wcss, silhouette_scores = calculate_metrics(data_scaled, k_values)\n",
    "\n",
    "        # Find elbow point\n",
    "        optimal_k: int = find_elbow_point(wcss)\n",
    "        print(f'El número óptimo de clusters es: {optimal_k}')\n",
    "\n",
    "        # Plot metrics\n",
    "        plot_metrics(k_values, wcss, silhouette_scores)\n",
    "\n",
    "        # Perform clustering with optimal number of clusters\n",
    "        if optimal_k != -1:\n",
    "            kmeans: KMeans = KMeans(k=optimal_k, seed=0)\n",
    "            model = kmeans.fit(data_scaled)\n",
    "            clusters: DataFrame = model.transform(data_scaled)\n",
    "\n",
    "            # Show clustering results\n",
    "            display(clusters)\n",
    "            return clusters\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {str(e)}\")\n",
    "        return spark.createDataFrame([], \"string\")\n",
    "\n",
    "clusters = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c41e8a-31b4-4554-9293-aceca88d8fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dmlzdWFsaXphdGlvbl9kYXRhID0gY2x1c3RlcnMuZHJvcCgiZmVhdHVyZXMiLCAic2NhbGVkX2ZlYXR1cmVzIikud2l0aENvbHVtblJlbmFtZWQoInByZWRpY3Rpb24iLCAiY2x1c3RlciIpCgpkaXNwbGF5KHZpc3VhbGl6YXRpb25fZGF0YSkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView1b05014\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView1b05014\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView1b05014\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView1b05014) SELECT `year`,`armed_forces`,`cluster` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView1b05014\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "cluster",
             "id": "column_3d97093e1"
            },
            "x": {
             "column": "year",
             "id": "column_5ae46ab155"
            },
            "y": [
             {
              "column": "armed_forces",
              "id": "column_5ae46ab145"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "armed_forces": {
             "type": "scatter",
             "yAxis": 0
            },
            "gnp": {
             "type": "scatter",
             "yAxis": 0
            },
            "unemployed_people": {
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "6c6931ea-6187-442b-86af-831bf04e49c4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "year",
           "type": "column"
          },
          {
           "column": "armed_forces",
           "type": "column"
          },
          {
           "column": "cluster",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dmlzdWFsaXphdGlvbl9kYXRhID0gY2x1c3RlcnMuZHJvcCgiZmVhdHVyZXMiLCAic2NhbGVkX2ZlYXR1cmVzIikud2l0aENvbHVtblJlbmFtZWQoInByZWRpY3Rpb24iLCAiY2x1c3RlciIpCgpkaXNwbGF5KHZpc3VhbGl6YXRpb25fZGF0YSkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2b2c07b\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2b2c07b\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2b2c07b\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2b2c07b) SELECT `unemployed_people`,`armed_forces`,`cluster`,`year` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2b2c07b\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 2",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "cluster",
             "id": "column_3d97093e2"
            },
            "size": {
             "column": "year",
             "id": "column_5ae46ab154"
            },
            "x": {
             "column": "unemployed_people",
             "id": "column_5ae46ab148"
            },
            "y": [
             {
              "column": "armed_forces",
              "id": "column_5ae46ab152"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "bubble",
           "hideXAxis": false,
           "legend": {
            "enabled": true,
            "placement": "auto",
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "armed_forces": {
             "type": "bubble",
             "yAxis": 0
            },
            "gnp": {
             "type": "bubble",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "area",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "85968bf1-38cd-4e06-9e94-7e5fa77bbbcd",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "unemployed_people",
           "type": "column"
          },
          {
           "column": "armed_forces",
           "type": "column"
          },
          {
           "column": "cluster",
           "type": "column"
          },
          {
           "column": "year",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization_data = clusters.drop(\"features\", \"scaled_features\").withColumnRenamed(\"prediction\", \"cluster\")\n",
    "\n",
    "display(visualization_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2c8668-82f1-4d6c-956d-18ce36d45123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "from pyspark.ml.evaluation import ClusteringEvaluator\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette_score = evaluator.evaluate(clusters)\nprint(f\"Silhouette Score: {silhouette_score}\")\n\nfrom pyspark.sql import Row\n\n# Crear un DataFrame de Spark con el Silhouette Score\nsilhouette_row = Row(\"metric\", \"value\")\nsilhouette_df = spark.createDataFrame([silhouette_row(\"Silhouette Score\", round(silhouette_score, 2))])\n\n# Mostrar el DataFrame con el Silhouette Score\ndisplay(silhouette_df)",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "countRow": false,
           "counterColName": "metric",
           "counterLabel": "",
           "formatTargetValue": false,
           "rowNumber": 1,
           "stringDecChar": ".",
           "stringDecimal": 0,
           "stringThouSep": ",",
           "targetColName": "value",
           "targetRowNumber": 1,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "660f0e1b-e7b8-4f3f-98c3-e3f9cf85c4c8",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette_score = evaluator.evaluate(clusters)\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "\n",
    "# Crear un DataFrame de Spark con el Silhouette Score\n",
    "silhouette_row = Row(\"metric\", \"value\")\n",
    "silhouette_df = spark.createDataFrame([silhouette_row(\"Silhouette Score\", round(silhouette_score, 2))])\n",
    "\n",
    "# Mostrar el DataFrame con el Silhouette Score\n",
    "display(silhouette_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "85968bf1-38cd-4e06-9e94-7e5fa77bbbcd",
       "elementType": "command",
       "guid": "888db572-b280-4a01-a93f-c6b64eea8ffb",
       "options": {
        "autoScaleImg": false,
        "scale": 0,
        "showTitle": false,
        "titleAlign": "center"
       },
       "position": {
        "height": 13,
        "width": 13,
        "x": 0,
        "y": 3,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "660f0e1b-e7b8-4f3f-98c3-e3f9cf85c4c8",
       "elementType": "command",
       "guid": "bb2de622-5c08-4aad-9a2a-31cc64087ed5",
       "options": {
        "autoScaleImg": false,
        "scale": 0,
        "showTitle": false,
        "titleAlign": "center"
       },
       "position": {
        "height": 3,
        "width": 13,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "9b102ac1-8d53-443b-8b96-db1517f49e29",
     "origId": 1539567030626359,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "KMeans Clustering with Spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
